{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9305d568-6e58-4aea-9467-5eb3341f7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opengeohub/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.3-CAPI-1.17.3) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from eumap.misc import find_files, nan_percentile, GoogleSheet, ttprint\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tool_kit import calc_ccc, accuracy_plot\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, HalvingGridSearchCV, KFold, GroupKFold\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # model parameters\n",
    "# import json\n",
    "# with open('/mnt/inca/soc_eu_model/data/006_params_annual.json', 'r') as file:\n",
    "#     params = json.load(file)\n",
    "\n",
    "folder = '/mnt/inca/soc_eu_model'\n",
    "df = pd.read_csv(f'{folder}/data/005.0_train.pnts_soc.csv',low_memory=False)\n",
    "\n",
    "# target\n",
    "train = df.loc[df['oc'].notna()]\n",
    "train = train.loc[train['oc']>5]\n",
    "train = train.loc[train['ref']!='nl.bis'] # show weird patterns\n",
    "train.loc[:,'oc_log1p'] = np.log1p(train['oc'])\n",
    "tgt = 'oc_log1p'\n",
    "\n",
    "# covariates\n",
    "with open(f'{folder}/SOC-EU/features/002_selected.covar_rank.freq.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "covs = [line.strip() for line in lines]\n",
    "train = train.dropna(subset=covs,how='any')\n",
    "\n",
    "spatial_cv_column = 'tile_id'\n",
    "cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe135b57-217a-4e97-9c03-14455c42e384",
   "metadata": {},
   "source": [
    "### Conduct hyperparameter tuning for different base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a35eda-9324-41bf-89a5-879e0ce6f64a",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb95e2d-9c21-4c55-ac64-df25d5b96070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:47:45] start parameter fine tuning for rf, training size: 129109\n",
      "n_iterations: 5\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 5\n",
      "min_resources_: 531\n",
      "max_resources_: 129109\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 540\n",
      "n_resources: 531\n",
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 180\n",
      "n_resources: 1593\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 60\n",
      "n_resources: 4779\n",
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 20\n",
      "n_resources: 14337\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 7\n",
      "n_resources: 43011\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[11:22:14] Finish fine tuning\n",
      "Best parameters found:  {'max_depth': 30, 'max_features': 0.3, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 800}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/mnt/inca/soc_eu_model/SOC-EU/model/002_model_rf.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GroupKFold\n",
    "import joblib\n",
    "\n",
    "# https://zillow.github.io/quantile-forest/user_guide/fit_predict.html#random-forest-predictions\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500, 800, 1000],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': [0.3, 0.5, 0.7, 'log2', 'sqrt'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "tune_rf = HalvingGridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90, \n",
    "    cv=cv,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ttprint(f'start parameter fine tuning for rf, training size: {len(train)}')\n",
    "tune_rf.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_rf.best_params_)\n",
    "joblib.dump(tune_rf.best_params_, f'{folder}/SOC-EU/model/001_best.params_rf.joblib')\n",
    "joblib.dump(tune_rf.best_estimator_, f'{folder}/SOC-EU/model/002_model_rf.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285870b8-2634-47d7-bbc5-839221ee8dca",
   "metadata": {},
   "source": [
    "#### lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d4c611-7eeb-49c5-a2ee-ef1c7858c472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:05] start parameter fine tuning for lightGBM, training size: 129109\n",
      "n_iterations: 7\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 7\n",
      "min_resources_: 177\n",
      "max_resources_: 129109\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 972\n",
      "n_resources: 177\n",
      "Fitting 5 folds for each of 972 candidates, totalling 4860 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 324\n",
      "n_resources: 531\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 108\n",
      "n_resources: 1593\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 36\n",
      "n_resources: 4779\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 12\n",
      "n_resources: 14337\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 4\n",
      "n_resources: 43011\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 2\n",
      "n_resources: 129033\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[02:09:05] Finish fine tuning\n",
      "Best parameters found:  {'boosting_type': 'gbdt', 'learning_rate': 0.01, 'max_depth': -1, 'min_child_samples': 20, 'n_estimators': 800, 'num_leaves': 31, 'subsample': 1.0, 'verbose': -1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/mnt/inca/soc_eu_model/SOC-EU/model/004_model_lgbm.joblib']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/index.html\n",
    "import lightgbm as lgb\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GroupKFold\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "# [LightGBM] [Fatal] Do not support special JSON characters in feature name.\n",
    "def clean_feature_names(df, covs):\n",
    "    df.columns = df.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "    clean_covs = [re.sub(r'[^\\w]', '_', i) for i in covs]\n",
    "    return df, clean_covs\n",
    "\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'dart'], # traditional Gradient Boosting Decision Tree VS. dropouts meet Multiple Additive Regression Trees (prevent overfitting)\n",
    "    'num_leaves': [31, 50, 80], # number of leaved in trees\n",
    "    'max_depth': [-1, 10, 20], # depth of a tree\n",
    "    'learning_rate': [0.01, 0.1], # shrinkage or step size, this parameter controls the impact of each tree on the final outcome\n",
    "    'n_estimators': [100, 500, 800], # number of boosting rounds\n",
    "    'subsample': [0.6, 0.8, 1.0], # fraction of samples to be used for each tree\n",
    "    'min_child_samples': [10,20,30], # minimum number of data points needed in a leaf\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "# HalvingGridSearchCV for tuning\n",
    "tune_lgbm = HalvingGridSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Clean feature names\n",
    "train_clean, covs_clean = clean_feature_names(train, covs)\n",
    "\n",
    "# Parameter tuning\n",
    "ttprint(f'start parameter fine tuning for lightGBM, training size: {len(train_clean)}')\n",
    "tune_lgbm.fit(train_clean[covs_clean], train_clean[tgt], groups=train[spatial_cv_column])\n",
    "ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_lgbm.best_params_)\n",
    "joblib.dump(tune_lgbm.best_params_, f'{folder}/SOC-EU/model/003_best.params_lgbm.joblib')\n",
    "joblib.dump(tune_lgbm.best_estimator_, f'{folder}/SOC-EU/model/004_model_lgbm.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0ce5f-c2a9-413f-b563-bba05291b9e9",
   "metadata": {},
   "source": [
    "#### ANN with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f0255-ca5f-4b0a-8a24-c7a6d5895be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:58:41] start grid search\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert DataFrame to PyTorch tensors\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(train[covs])\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(train[[tgt]]).reshape(-1)\n",
    "\n",
    "X = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y = torch.tensor(y_scaled, dtype=torch.float32).reshape(-1, 1)\n",
    "groups = train[spatial_cv_column].values\n",
    "\n",
    "# Define PyTorch model class\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, units, layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        layers_list = [nn.Linear(X.shape[1], units), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        for _ in range(1, layers):\n",
    "            layers_list += [nn.Linear(units, units), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        layers_list.append(nn.Linear(units, 1))\n",
    "        self.net = nn.Sequential(*layers_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Skorch wrapper\n",
    "def skorch_model(units=64, layers=1, dropout_rate=0.2, learning_rate=0.001):\n",
    "    return NeuralNetRegressor(\n",
    "        RegressionModel,\n",
    "        module__units=units,\n",
    "        module__layers=layers,\n",
    "        module__dropout_rate=dropout_rate,\n",
    "        max_epochs=10,  # This value will be overwritten by GridSearchCV\n",
    "        lr=learning_rate,\n",
    "        optimizer=optim.Adam,\n",
    "        criterion=nn.MSELoss,\n",
    "        batch_size=64,  # This value will be overwritten by GridSearchCV\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    'module__units': [64, 128, 256],\n",
    "    'module__layers': [2,4,6],\n",
    "    'module__dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'lr': [0.0005, 0.001, 0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "    'batch_size': [64, 128]\n",
    "}\n",
    "\n",
    "ttprint('start grid search')\n",
    "cv = GroupKFold(n_splits=3)\n",
    "grid = GridSearchCV(estimator=skorch_model(), param_grid=param_grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X, y, groups=groups)\n",
    "ttprint('finish tuning')\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "joblib.dump(grid_result.best_params_, f'{folder}/SOC-EU/model/005_best.params_ann.joblib')\n",
    "joblib.dump(grid_result.best_estimator_,  f'{folder}/SOC-EU/model/006_model_ann.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f17f8-cba1-44a9-ac1f-3514db9c8060",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d6dd3-ee4f-477f-8cd9-dc522edf2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "\n",
    "# find the best boundary (hyperplane) that separates data points of different classes in the feature space\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],  # regularization parameter, lower values of C lead to a smaller margin in the separating hyperplane\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # type of hyperplane used to separate the data\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],  # the influence of a single training example\n",
    "    'degree': [2, 3, 4]  # degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "}\n",
    "\n",
    "# Set up the HalvingGridSearchCV for SVM\n",
    "tune_svm = HalvingGridSearchCV(\n",
    "    estimator=SVR(),  \n",
    "    param_grid=param_grid_svm,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90,  \n",
    "    cv=cv, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Assuming you have defined train, covs, tgt, and spatial_cv_column appropriately\n",
    "print(f'start parameter fine tuning for SVM, training size: {len(train)}')\n",
    "tune_svm.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "print(\"Finish fine tuning\\nBest parameters found: \", tune_svm.best_params_)\n",
    "\n",
    "# Save the best parameters and the best estimator\n",
    "joblib.dump(tune_svm.best_params_, f'{folder}/SOC-EU/model/009_best.params_svm.joblib')\n",
    "joblib.dump(tune_svm.best_estimator_, f'{folder}/SOC-EU/model/010_model_svm.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b365c5-dc81-4fd1-a13c-e07c6b3ba514",
   "metadata": {},
   "source": [
    "### lrboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da035a-5f5c-4dd1-807a-b798b36aa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/lrboost/\n",
    "\n",
    "from lrboost import LRBoostRegressor\n",
    "\n",
    "ttprint(f'fit linear regression Boost regressor')\n",
    "lrb = LRBoostRegressor(primary_model=RidgeCV(), secondary_model=RandomForestRegressor())\n",
    "lrb = LRBoostRegressor.fit(train[covs], train[tgt])\n",
    "ttprint(f'finish fitting linear regression Boost regressor')\n",
    "\n",
    "joblib.dump(lrb, f'{folder}/SOC-EU/model/011_model_lrb.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2c86f-3d13-456c-9452-a9438c9b1544",
   "metadata": {},
   "source": [
    "#### Cubist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a8008-10f8-47e5-9607-23fc2120375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cubist import Cubist\n",
    "# https://pypi.org/project/cubist/\n",
    "# rule-based predictive model\n",
    "param_cubist = {\n",
    "    'n_rules': [100, 300, 500], # number of rules to be generated\n",
    "    'n_committees': [1, 5, 10], # committee: ensembles of models\n",
    "    'neighbors': [None, 3, 6, 9], # number of nearest neighbors to use when making a prediction\n",
    "    'unbiased': [False, True], # whether or not to use an unbiased method of rule generation\n",
    "    'extrapolation': [0.02, 0.05], # limits the extent to which predictions can extrapolate beyond the range of the training data, a fraction of the total range of the target variable\n",
    "    'sample': [None, 0.1, 0.5], # fraction of the training data used in building each model\n",
    "    'cv': [10]\n",
    "}\n",
    "\n",
    "tune_cubist = HalvingGridSearchCV(\n",
    "    estimator=Cubist(),\n",
    "    param_grid=param_cubist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Start fine-tuning process\n",
    "ttprint('start fine tuning cubist')\n",
    "tune_cubist.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "ttprint('finish fitting')\n",
    "\n",
    "print(\"Best parameters:\", tune_cubist.best_params_)\n",
    "joblib.dump(tune_cubist.best_params_, f'{folder}/SOC-EU/model/007_best.params_ann.joblib')\n",
    "joblib.dump(tune_cubist.best_estimator_,  f'{folder}/SOC-EU/model/008_model_cubist.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78368e31-8ddd-4e74-ba1b-e845b601411d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
