{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9305d568-6e58-4aea-9467-5eb3341f7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCleaning oc\u001b[0m\n",
      "originally with 177792 rows\n",
      "18009 (10.13%) rows with NaN, from ['portugal.infosolo' 'swiss.nabo' 'foregs' 'nl.bis' 'estonia.kese' 'ukceh'\n",
      " 'SoDaH' 'gemas' 'croatia.multione' 'netherland.BHR-P' 'Castilla.y.Leon'\n",
      " 'geocradle' 'MarSOC' 'basque' 'LUCAS']\n",
      "266 (0.15%) rows with invalid strings, from ['LUCAS']\n",
      "1405 (0.79%) rows with oc < 0, from ['nl.bis' 'Czech' 'LUCAS' 'Wales.GMEP']\n",
      "25 (0.01%) rows with oc > limit values, from ['estonia.kese']\n",
      "158087 valid data records left\n",
      "\n",
      "152550 valid data records left after cleaning covariates\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "# covariates\n",
    "with open('/mnt/inca/soc_eu_model/data/005_covar_annual.static.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "covs = [line.strip() for line in lines]\n",
    "\n",
    "# training data\n",
    "df = pd.read_csv('/mnt/inca/soc_eu_model/data/002_covar_overlayed.csv',low_memory=False)\n",
    "def clean_prop(df, prop, limit):\n",
    "    print(f'\\033[1mCleaning {prop}\\033[0m')\n",
    "    tot = len(df)\n",
    "    print(f'originally with {tot} rows')\n",
    "    # Clean NaN\n",
    "    num = df[prop].isna().sum()\n",
    "    ccol = df.loc[df[prop].isna()]['ref'].unique()\n",
    "    print(f'{num} ({num/tot*100:.2f}%) rows with NaN, from {ccol}')\n",
    "    df = df.dropna(subset=[prop])\n",
    "    \n",
    "    # check if there are string values that cannot be converted to numerical values,\n",
    "    # usually it's <LOD (limit of detection), such as '<6', '<LOD', etc\n",
    "#     df.loc[:,prop] = pd.to_numeric(df.loc[:,prop], errors='coerce')\n",
    "    df[prop] = pd.to_numeric(df[prop], errors='coerce')\n",
    "    num = df[prop].isna().sum()\n",
    "    ccol = df.loc[df[prop].isna()]['ref'].unique()\n",
    "    print(f'{num} ({num/tot*100:.2f}%) rows with invalid strings, from {ccol}')\n",
    "    df = df.dropna(subset=[prop])\n",
    "    \n",
    "    # Check for values below 0, which are invalid for all properties\n",
    "    num = len(df.loc[df[prop] < 0])\n",
    "    ccol = df.loc[df[prop] < 0]['ref'].unique()\n",
    "    print(f'{num} ({num/tot*100:.2f}%) rows with {prop} < 0, from {ccol}')\n",
    "    df = df[df[prop] >= 0]\n",
    "    \n",
    "    # check for values higher than plausible limit\n",
    "    if limit:\n",
    "        num = len(df.loc[df[prop]>limit])\n",
    "        ccol = df.loc[df[prop]>limit]['ref'].unique()\n",
    "        print(f'{num} ({num/tot*100:.2f}%) rows with {prop} > limit values, from {ccol}')\n",
    "        df = df[df[prop] < limit]\n",
    "    \n",
    "    print(f'{len(df)} valid data records left')\n",
    "    return df\n",
    "\n",
    "tgt = 'oc'\n",
    "dff = clean_prop(df,tgt,1000)\n",
    "print()\n",
    "dff = dff.dropna(subset=covs, how='any')\n",
    "print(f'{len(dff)} valid data records left after cleaning covariates')\n",
    "dff.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4218061-34d5-4d03-aa33-ae9496514b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mapie._typing import NDArray\n",
    "from mapie.metrics import regression_coverage_score\n",
    "from mapie.regression import MapieQuantileRegressor, MapieRegressor\n",
    "\n",
    "train_set, test_set = train_test_split(dff, test_size=0.4, random_state=42)\n",
    "vld_set, clb_set = train_test_split(test_set, test_size=0.5, random_state=42)\n",
    "\n",
    "mapie = MapieRegressor(rf, method='plus', cv=10)\n",
    "\n",
    "mapie.fit(clb_set[covs], clb_set[tgt])\n",
    "\n",
    "# Evaluate prediction and coverage level on testing set\n",
    "y_pred, y_pis = mapie.predict(test_set[covs], alpha=0.1)\n",
    "coverage = regression_coverage_score(test_set[tgt], y_pis[:, 0, 0], y_pis[:, 1, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a35eda-9324-41bf-89a5-879e0ce6f64a",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb95e2d-9c21-4c55-ac64-df25d5b96070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:06] fit random forest\n",
      "[18:50:19] finish fitting\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(dff, test_size=0.4, random_state=42)\n",
    "cal_set, val_set = train_test_split(test_set, test_size=0.5, random_state=42)\n",
    "\n",
    "from joblib import dump, load\n",
    "rf = load('/mnt/inca/soc_eu_model/data/008_model_rf.joblib')\n",
    "\n",
    "from mapie import MapieRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "mapie = MapieRegressor(estimator=rf, method='cv_plus', cv=cv)\n",
    "mapie.fit(cal_set[covs],cal_set[tgt])\n",
    "\n",
    "y_pred, y_pred_intervals = mapie.predict(X_test, alpha=0.05)\n",
    "\n",
    "# Display or use the prediction intervals\n",
    "for i, (interval) in enumerate(y_pred_intervals):\n",
    "    print(f\"Prediction Interval for observation {i}: {interval[0]:.2f} to {interval[1]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0ce5f-c2a9-413f-b563-bba05291b9e9",
   "metadata": {},
   "source": [
    "### Multi layer percepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f0255-ca5f-4b0a-8a24-c7a6d5895be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:46] start fine tuning ann\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_ann = {\n",
    "    'mlp__hidden_layer_sizes': [(100,), (150,), (100, 50)],\n",
    "    'mlp__activation': ['tanh', 'relu'],\n",
    "    'mlp__solver': ['adam'],\n",
    "    'mlp__alpha': [0.0001, 0.001],  # Regularization term\n",
    "    'mlp__learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Feature scaling\n",
    "    ('mlp', MLPRegressor(max_iter=300, random_state=42))\n",
    "])\n",
    "\n",
    "tune_ann = HalvingGridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_ann,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "ttprint(f'start fine tuning ann')\n",
    "tune_ann.fit(train_set[covs], train_set[tgt])\n",
    "ttprint(f'finish fine tuning')\n",
    "ann = tune_ann.best_estimator_\n",
    "dump(ann, '/mnt/inca/soc_eu_model/data/009_model_ann.joblib')\n",
    "\n",
    "print(\"Best parameters:\", tune_ann.best_params_)\n",
    "dump(tune_ann.best_parameters, '/mnt/inca/soc_eu_model/data/010_param_ann.joblib')\n",
    "\n",
    "y_pred_ann = ann.predict(test_set[covs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeddfcf-7fe2-4de6-b892-82e96d7c88bd",
   "metadata": {},
   "source": [
    "### Cubist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f45b52-a0a3-4b18-9ba0-0b8968cbf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_cubist = {\n",
    "    'cubist__n_rules': [100, 300, 500],\n",
    "    'cubist__n_committees': [1, 5, 10],\n",
    "    'cubist__neighbors': [None, 5, 9],\n",
    "    'cubist__unbiased': [False, True],\n",
    "    'cubist__auto': [True, False],\n",
    "    'cubist__extrapolation': [0.02, 0.05],\n",
    "    'cubist__sample': [None, 0.1, 0.5],\n",
    "    'cubist__cv': 10\n",
    "}\n",
    "\n",
    "tune_cubist = HalvingGridSearchCV(\n",
    "    estimator=Cubist(),\n",
    "    param_grid=param_cubist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Start fine-tuning process\n",
    "ttprint('start fine tuning cubist')\n",
    "tune_cubist.fit(train_set[covs], train_set[tgt])\n",
    "ttprint('finish fitting')\n",
    "\n",
    "cubist = tune_cubist.best_estimator_\n",
    "dump(cubist, '/mnt/inca/soc_eu_model/data/011_model_cubist.joblib')\n",
    "\n",
    "print(\"Best parameters:\", tune_cubist.best_params_)\n",
    "dump(tune_cubist.best_params_, '/mnt/inca/soc_eu_model/data/012_param_cubist.joblib')\n",
    "\n",
    "y_pred_cubist = cubist.predict(test_set[covs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b365c5-dc81-4fd1-a13c-e07c6b3ba514",
   "metadata": {},
   "source": [
    "### lrb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da035a-5f5c-4dd1-807a-b798b36aa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttprint(f'fit linear regression Boost regressor')\n",
    "from lrboost import LRBoostRegressor\n",
    "lrb = LRBoostRegressor().fit(train_set[covs], train_set[tgt])\n",
    "y_pred_lrb = lrb.predict(test_set[covs], detail=True)\n",
    "ttprint(f'finish fitting linear regression Boost regressor')\n",
    "\n",
    "dump(lrb, '/mnt/inca/soc_eu_model/data/011_model_lrb.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f8ce8-66c6-4cd1-b66b-4f34b0bc7267",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70840e4-9828-46e5-88fa-1cf0a7585836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calc_ccc(y_true, y_pred):\n",
    "    pearson_corr = pearsonr(y_true, y_pred)[0]\n",
    "    mean_true = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    var_true = np.var(y_true)\n",
    "    var_pred = np.var(y_pred)\n",
    "    ccc = (2 * pearson_corr * np.sqrt(var_true) * np.sqrt(var_pred)) / (var_true + var_pred + (mean_true - mean_pred)**2)\n",
    "    return ccc\n",
    "    \n",
    "def accuracy_plot(y_test, y_pred, title_text):\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    ccc = calc_ccc(y_test, y_pred)\n",
    "\n",
    "    matplotlib.rcParams.update({'font.size': 16})\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    fig.suptitle(title_text, fontsize=20, fontweight='bold')\n",
    "    plt.title(f'R2={r2:.2f}, rmse={rmse:.4f}, ccc={ccc:.2f}')\n",
    "    plt.hexbin(y_test, y_pred, gridsize=(300, 300) , cmap = 'plasma_r', mincnt=1, vmax = 200)#, xscale =25, yscale = 25)\n",
    "    \n",
    "    plt.xlabel('SOC - test'), plt.ylabel('SOC - pred')\n",
    "\n",
    "    # square plot\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('auto', adjustable='box')\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    #diagonal \n",
    "    ax.plot([-10, 300], [-10, 300], \"-k\", alpha = .5)\n",
    "    ax.set_xlim(-5,200)\n",
    "    ax.set_ylim(-5,200)\n",
    "    \n",
    "    # Create new axes according to image position\n",
    "    cax = fig.add_axes([ax.get_position().x1+0.05,\n",
    "                        ax.get_position().y0,\n",
    "                        0.02,\n",
    "                        ax.get_position().height])\n",
    "\n",
    "    # Plot vertical colorbar\n",
    "    cb = plt.colorbar(cax=cax)\n",
    "    #cb.outline.set_visible(False)\n",
    "    #cb.set_ticks([1,100,200,300])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "accuracy_plot(test_set[tgt], y_pred_rf, title_text='RF')\n",
    "accuracy_plot(test_set[tgt], y_pred_lrb['final_prediction'], title_text='LRB')\n",
    "accuracy_plot(test_set[tgt], y_pred_cubist, title_text='Cubist')\n",
    "accuracy_plot(test_set[tgt], y_pred_ann, title_text='MLP (NN)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb005f2-851a-4c2f-ac0d-fa3b882e2df3",
   "metadata": {},
   "source": [
    "### ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d0ba9-5304-4330-94ff-8d5344c80d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubist\n",
    "ann\n",
    "rf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
