{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034dea9b-c505-4ea5-8347-a5875d2b21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from eumap.misc import find_files, nan_percentile, GoogleSheet, ttprint\n",
    "from eumap.raster import read_rasters, save_rasters\n",
    "from eumap.mapper import SpaceOverlay\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from minio import Minio\n",
    "import rasterio\n",
    "import pyproj\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "# os.environ['PROJ_LIB'] = '/opt/conda/share/proj'\n",
    "folder = '/mnt/primus/xuemeng_tmp_harbour/soc'\n",
    "\n",
    "# /home/opengeohub/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56ee2b-daef-45d1-9310-f67a7c41dc95",
   "metadata": {},
   "source": [
    "### check if what is need to be overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbf9596-28b0-423a-b078-142146f8f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from geopandas import gpd\n",
    "\n",
    "# def get_data_to_be_overlayed(whole=False):\n",
    "#     df4326 = gpd.read_file(f'{folder}/data/soil_overlay.4326.gpkg')\n",
    "#     df3035 = gpd.read_file(f'{folder}/data/soil_overlay.3035.gpkg')\n",
    "    \n",
    "#     keys = ['sample_id', 'lat', 'lon', 'time', 'hzn_top', 'hzn_btm', 'ref']\n",
    "#     if whole:\n",
    "#         return df4326,df3035\n",
    "#     else:\n",
    "#         new4326 = gpd.read_file(f'{folder_path}/data/soil_overlay.4326_v2.gpkg')\n",
    "#         merge4326 = pd.merge(df4326, new4326, on=keys, how='outer', indicator=True)\n",
    "#         different_4326 = merge4326[merge4326['_merge'] != 'both']\n",
    "        \n",
    "#         new3035 = gpd.read_file(f'{folder_path}/data/soil_overlay.3035_v2.gpkg')\n",
    "#         merge3035 = pd.merge(df3035, new3035, on=keys, how='outer', indicator=True)\n",
    "#         different_3035 = merge3035[merge3035['_merge'] != 'both']\n",
    "#         return different_4326,different_3035\n",
    "\n",
    "# df4326, df3035 = get_data_to_be_overlayed(whole=True)     \n",
    "\n",
    "# # create gpkg\n",
    "added_covar = 1 # wether overlay from scratch or from overlayed version\n",
    "\n",
    "if added_covar:\n",
    "    df = pd.read_csv(f'{folder}/data/test_covar_overlayed.csv', low_memory=False)\n",
    "else:\n",
    "    df = pd.read_csv(f'{folder}/data/000_soil.full_qa.controlled.csv', low_memory=False)\n",
    "\n",
    "    \n",
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "\n",
    "df4326 = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "df3035 = df4326.to_crs(\"EPSG:3035\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1960bf-048d-456c-94b2-dacc0cc2a9cc",
   "metadata": {},
   "source": [
    "### generate overlay links\n",
    "- read in the files specified by Google Sheet\n",
    "- convert the files into readable linkes for overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4113b0ed-bb82-4c0d-a6e8-0f5ccb9e949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opengeohub/.local/lib/python3.8/site-packages/gspread/auth.py:335: DeprecationWarning: [Deprecated][in version 6.0.0]: client_factory will be replaced by gspread.http_client types\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# read in potential usable overlay files\n",
    "key_file = '/mnt/inca/soc_eu_model/gaia-319808-913d36b5fca4.json'\n",
    "url = 'https://docs.google.com/spreadsheets/d/1eIoPAvWM5jrhLrr25jwguAIR0YxOh3f5-CdXwpcOIz8/edit#gid=0'\n",
    "\n",
    "gsheet = GoogleSheet(key_file, url)\n",
    "covar = gsheet.covar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df5670e-bdfc-4e4b-a08a-b409349d4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate file paths by year, and check if the urls are valid\n",
    "def generate_overlay_path(row,year,filt=None):\n",
    "            \n",
    "    # determine if static variable\n",
    "    if row['temporal resolution'] == 'static':\n",
    "        return [row['path']],[row['path']]\n",
    "    \n",
    "    if row['temporal resolution'] == 'long term':\n",
    "        perc_list = row['perc'].split(',')\n",
    "        output_paths = [row['path'].replace('{perc}', perc) for perc in perc_list]\n",
    "        return output_paths, output_paths\n",
    "        \n",
    "    # determine if the year is ahead of the availibility of the variable\n",
    "    if year>int(row['end year']):\n",
    "        year = int(row['end year'])\n",
    "    \n",
    "    # determine if it's an annual variable or (bi)monthly variable\n",
    "    if '{start_m}' not in row['path']:\n",
    "        output_paths = [row['path'].replace('{year}',f'{int(year)}')]\n",
    "    else:\n",
    "        output_paths = []\n",
    "        start_list = row['start_m'].split(', ')\n",
    "        end_list = row['end_m'].split(', ')\n",
    "        output_paths = [row['path'].replace('{year}',f'{int(year)}').replace('{start_m}',start_list[i]).replace('{end_m}',end_list[i]) for i in range(len(end_list))]\n",
    "    \n",
    "    if '{perc}' in row['path']:\n",
    "        perc_list = row['perc'].split(',')\n",
    "        output_paths = [p.replace('{perc}', perc) for p in output_paths for perc in perc_list]\n",
    "        \n",
    "    if (row['leap year'] == '1') & (year%4==0):\n",
    "        output_paths = [p.replace('0228', '0229') if '0228' in p else p for p in output_paths]\n",
    "    \n",
    "    return output_paths, [i.replace(str(int(year)),'{year}') for i in output_paths]\n",
    "    \n",
    "def check_path(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        # Check if the status code is not 200 (OK). You might want to specifically check for 404 or other error codes.\n",
    "        if response.status_code == 404:\n",
    "            print(f\"{url} returned HTTP 404 Not Found\")\n",
    "            return url\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"{url} returned HTTP {response.status_code}\")\n",
    "            return url\n",
    "        return None  # URL is fine (HTTP 200), or you might want to handle redirections (HTTP 3xx) separately if needed.\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {str(e)}\")\n",
    "        return url\n",
    "    \n",
    "# # check function validity\n",
    "# # generate paths\n",
    "# paths = []\n",
    "# for index,row in covar.iterrows():\n",
    "#     paths.extend(generate_overlay_path(row,2000))\n",
    "    \n",
    "pathl = []\n",
    "namel = []\n",
    "year = 2000\n",
    "for index,row in covar.iterrows():\n",
    "    if row['need update in overlay']=='1':\n",
    "        paths, names = generate_overlay_path(row, year)\n",
    "        pathl.extend(paths)\n",
    "        namel.extend(names)\n",
    "    \n",
    "for i in pathl:\n",
    "    check_path(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3928c7ad-de83-490a-a14f-e4a78f15125c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n",
      "461\n"
     ]
    }
   ],
   "source": [
    "print(len(df3035.columns))\n",
    "dropl = []\n",
    "for i in namel:\n",
    "    if i in df3035.columns:\n",
    "        dropl.append(i)\n",
    "df3035 = df3035.drop(columns=dropl)\n",
    "print(len(df3035.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194849f7-1c3a-4ce8-9ea7-fc55c6b2d937",
   "metadata": {},
   "source": [
    "#### mend overlay on long term covars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1591c8f7-48e2-476d-b6ae-296c569d1c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:20:58] start overlaying for static, size: 394643, column num: 4\n"
     ]
    }
   ],
   "source": [
    "path_stem = [i.split('/')[-1][0:-4] for i in pathl]\n",
    "namel = [i.split('/')[-1][0:-4] for i in namel]\n",
    "name_mapping = dict(zip(path_stem,namel))\n",
    "    \n",
    "df_overlay = df3035\n",
    "        \n",
    "ttprint(f'start overlaying for static, size: {len(df_overlay)}, column num: {len(pathl)}')\n",
    "pathl = [Path(ii) for ii in pathl]\n",
    "dfo = SpaceOverlay(df_overlay, fn_layers=pathl, max_workers=90, verbose=False)\n",
    "temp = dfo.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de10f7-b69b-4e7e-b644-bfe3e70c52fd",
   "metadata": {},
   "source": [
    "### overlay year by year\n",
    "- divide soil data by year\n",
    "- overlay the soil data in each year with corresponding covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe2ca97e-5ab6-4de1-aaff-893de6117eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:12:51] start overlaying for year 2000, size: 26304, column num: 31\n",
      "[12:13:28] finish overlaying for year 2000\n",
      "[12:13:28] start overlaying for year 2001, size: 12777, column num: 31\n",
      "[12:13:54] finish overlaying for year 2001\n",
      "[12:13:54] start overlaying for year 2002, size: 11718, column num: 31\n",
      "[12:14:18] finish overlaying for year 2002\n",
      "[12:14:18] start overlaying for year 2003, size: 13497, column num: 31\n",
      "[12:14:45] finish overlaying for year 2003\n",
      "[12:14:45] start overlaying for year 2004, size: 7649, column num: 31\n",
      "[12:15:01] finish overlaying for year 2004\n",
      "[12:15:01] start overlaying for year 2005, size: 12987, column num: 31\n",
      "[12:15:27] finish overlaying for year 2005\n",
      "[12:15:27] start overlaying for year 2006, size: 13075, column num: 31\n",
      "[12:15:50] finish overlaying for year 2006\n",
      "[12:15:50] start overlaying for year 2007, size: 18561, column num: 31\n",
      "[12:16:19] finish overlaying for year 2007\n",
      "[12:16:19] start overlaying for year 2008, size: 11983, column num: 31\n",
      "[12:17:01] finish overlaying for year 2008\n",
      "[12:17:01] start overlaying for year 2009, size: 25942, column num: 31\n",
      "[12:17:58] finish overlaying for year 2009\n",
      "[12:17:58] start overlaying for year 2010, size: 12330, column num: 31\n",
      "[12:18:22] finish overlaying for year 2010\n",
      "[12:18:23] start overlaying for year 2011, size: 15374, column num: 31\n",
      "[12:18:48] finish overlaying for year 2011\n",
      "[12:18:48] start overlaying for year 2012, size: 26062, column num: 31\n",
      "[12:19:33] finish overlaying for year 2012\n",
      "[12:19:33] start overlaying for year 2013, size: 21428, column num: 31\n",
      "[12:20:11] finish overlaying for year 2013\n",
      "[12:20:11] start overlaying for year 2014, size: 16922, column num: 31\n",
      "[12:20:47] finish overlaying for year 2014\n",
      "[12:20:47] start overlaying for year 2015, size: 33628, column num: 31\n",
      "[12:21:57] finish overlaying for year 2015\n",
      "[12:21:57] start overlaying for year 2016, size: 16177, column num: 31\n",
      "[12:22:34] finish overlaying for year 2016\n",
      "[12:22:34] start overlaying for year 2017, size: 15843, column num: 31\n",
      "[12:23:01] finish overlaying for year 2017\n",
      "[12:23:01] start overlaying for year 2018, size: 42918, column num: 31\n",
      "[12:24:11] finish overlaying for year 2018\n",
      "[12:24:11] start overlaying for year 2019, size: 15088, column num: 31\n",
      "[12:24:37] finish overlaying for year 2019\n",
      "[12:24:37] start overlaying for year 2020, size: 10200, column num: 31\n",
      "[12:25:00] finish overlaying for year 2020\n",
      "[12:25:01] start overlaying for year 2021, size: 10886, column num: 31\n",
      "[12:25:21] finish overlaying for year 2021\n",
      "[12:25:22] start overlaying for year 2022, size: 3294, column num: 31\n",
      "[12:25:37] finish overlaying for year 2022\n",
      "no data for year 2023\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# epsg 3035 overlay\n",
    "co3035 = covar.loc[covar['epsg']=='3035']\n",
    "\n",
    "# for year in np.arange(2000,2023,1):\n",
    "for year in np.arange(2000, 2024, 1):\n",
    "    pathl = []\n",
    "    namel = []\n",
    "    for index,row in co3035.iterrows():\n",
    "        if row['need update in overlay']=='1':\n",
    "            paths, names = generate_overlay_path(row, year)\n",
    "            pathl.extend(paths)\n",
    "            namel.extend(names)\n",
    "            # path3035.extend(generate_overlay_path(row,year))\n",
    "    for iii in pathl:\n",
    "        check_path(iii)\n",
    "    path_stem = [i.split('/')[-1][0:-4] for i in pathl]\n",
    "    namel = [i.split('/')[-1][0:-4] for i in namel]\n",
    "    name_mapping = dict(zip(path_stem,namel))\n",
    "    \n",
    "    df_overlay = df3035.loc[df3035['time']==year]\n",
    "    if len(df_overlay)==0:\n",
    "        print(f'no data for year {year}')\n",
    "        continue\n",
    "        \n",
    "    ttprint(f'start overlaying for year {str(int(year))}, size: {len(df_overlay)}, column num: {len(pathl)}')\n",
    "    pathl = [Path(ii) for ii in pathl]\n",
    "    dfo = SpaceOverlay(df_overlay, fn_layers=pathl, max_workers=90, verbose=False)\n",
    "    temp = dfo.run()\n",
    "    \n",
    "    temp = temp.rename(columns=name_mapping)\n",
    "    temp=temp.drop(columns=['overlay_id'])\n",
    "    # temp = pd.read_csv(f'/mnt/inca/soc_eu_model/overlay_intermediate/dft_{str(int(tt))}_3035.csv',index=False)\n",
    "    temp.to_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}.mend_3035.csv',index=False)\n",
    "    ttprint(f'finish overlaying for year {str(int(year))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdaee12-dad4-4f62-a12c-3e4f1981495b",
   "metadata": {},
   "source": [
    "### assemble the overlayed annual datasets\n",
    "- read in the overlayed soil data (with covariates) for each year\n",
    "- combine them into a whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f82ae897-faee-42eb-8d23-b34806425c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000, 491, 26304\n",
      "2001, 491, 12777\n",
      "2002, 491, 11718\n",
      "2003, 491, 13497\n",
      "2004, 491, 7649\n",
      "2005, 491, 12987\n",
      "2006, 491, 13075\n",
      "2007, 491, 18561\n",
      "2008, 491, 11983\n",
      "2009, 491, 25942\n",
      "2010, 491, 12330\n",
      "2011, 491, 15374\n",
      "2012, 491, 26062\n",
      "2013, 491, 21428\n",
      "2014, 491, 16922\n",
      "2015, 491, 33628\n",
      "2016, 491, 16177\n",
      "2017, 491, 15843\n",
      "2018, 491, 42918\n",
      "2019, 491, 15088\n",
      "2020, 491, 10200\n",
      "2021, 491, 10886\n",
      "2022, 491, 3294\n",
      "whole 3035, cols:491, size:394643\n"
     ]
    }
   ],
   "source": [
    "## read in 3035 datasets\n",
    "mended = '.mend'\n",
    "tl = []\n",
    "for year in np.arange(2000,2023,1):\n",
    "    temp = pd.read_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}{mended}_3035.csv',low_memory=False)\n",
    "    print(f'{year}, {len(temp.columns)}, {len(temp)}')\n",
    "    tl.append(temp)\n",
    "\n",
    "df3035 = pd.concat(tl)\n",
    "print(f'whole 3035, cols:{len(df3035.columns)}, size:{len(df3035)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef6cadcd-30b5-4501-b66f-9600f95fd9e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7101201338199513 2000 100.0\n",
      "0.8049620411677233 2001 100.0\n",
      "0.6483188257381806 2002 100.0\n",
      "0.4095724975920575 2003 100.0\n",
      "0.7522551967577461 2004 25.0\n",
      "0.7871717871717872 2005 50.0\n",
      "0.6450478011472275 2006 25.0\n",
      "0.6639189698830882 2007 100.0\n",
      "0.6091129099557707 2008 25.0\n",
      "0.5784442217253875 2009 50.0\n",
      "0.5128953771289537 2010 25.0\n",
      "0.5467022245349291 2011 100.0\n",
      "0.6755429360755122 2012 25.0\n",
      "0.4654657457532201 2013 50.0\n",
      "0.6197848954024348 2014 25.0\n",
      "0.578684429641965 2015 100.0\n",
      "0.6435062125239538 2016 25.0\n",
      "0.4782553809253298 2017 50.0\n",
      "0.5796868446805536 2018 25.0\n",
      "0.7391304347826086 2019 100.0\n",
      "0.7472549019607844 2020 100.0\n",
      "0.6805989344111704 2021 100.0\n",
      "0.8269581056466302 2022 100.0\n"
     ]
    }
   ],
   "source": [
    "col = 'cropland.extent_glad.interpolate_p_30m_s_{year}0101_{year}1231_eu_epsg.3035_v20240604'\n",
    "for year in np.arange(2000,2023,1):\n",
    "    temp = df3035.loc[df3035['time']==year]\n",
    "    aaa = temp[col].isna().sum()/len(temp)\n",
    "    print(aaa, year, temp[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb140667-256c-4a4b-95bc-0c72733c32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcv_wilderness_li2022.human.footprint_p_1km_s0..0cm_{year}_v16022022 17059\n",
      "fgd_chelsa_m_1km_s_19810101_20101231_eu_epsg.3035_v20240531 264869\n",
      "fcf_chelsa_m_1km_s_19810101_20101231_eu_epsg.3035_v20240531 198538\n",
      "lgd_chelsa_m_1km_s_19810101_20101231_eu_epsg.3035_v20240531 264869\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0101_{year}0131_eu_epsg.3035_v20230619 99338\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0201_{year}0228_eu_epsg.3035_v20230619 99686\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0301_{year}0331_eu_epsg.3035_v20230619 110373\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0501_{year}0531_eu_epsg.3035_v20230619 72025\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0401_{year}0430_eu_epsg.3035_v20230619 113298\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0701_{year}0731_eu_epsg.3035_v20230619 91872\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0801_{year}0831_eu_epsg.3035_v20230619 47493\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0601_{year}0630_eu_epsg.3035_v20230619 16928\n",
      "wv_mcd19a2v061.seasconv_sd_1km_s_{year}1101_{year}1130_eu_epsg.3035_v20230619 10892\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}1101_{year}1130_eu_epsg.3035_v20230619 93103\n",
      "wv_mcd19a2v061.seasconv_sd_1km_s_{year}0901_{year}0930_eu_epsg.3035_v20230619 10892\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}1001_{year}1031_eu_epsg.3035_v20230619 76745\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}0901_{year}0930_eu_epsg.3035_v20230619 81002\n",
      "wv_mcd19a2v061.seasconv_m_1km_s_{year}1201_{year}1231_eu_epsg.3035_v20230619 134419\n",
      "cropland.extent_glad.interpolate_p_30m_s_{year}0101_{year}1231_eu_epsg.3035_v20240604 245158\n",
      "remove covariates with more than 2% data unavailable\n"
     ]
    }
   ],
   "source": [
    "dff = df3035\n",
    "meta_list = ['id', 'lat', 'lon', 'time', 'hzn_top', 'hzn_btm', 'ref', 'nuts0', 'oc',\n",
    "       'ph_h2o', 'ph_cacl2', 'bulk_density', 'clay', 'silt', 'sand', 'caco3',\n",
    "       'N', 'K', 'P', 'CEC', 'EC', 'oc_qa', 'N_qa', 'caco3_qa',\n",
    "       'bulk_density_qa', 'clay_qa', 'silt_qa', 'sand_qa', 'ph_h2o_qa',\n",
    "       'ph_cacl2_qa', 'P_qa', 'K_qa', 'EC_qa', 'CEC_qa', 'geometry']\n",
    "### check covariates availability\n",
    "drop_list = []\n",
    "for col in dff.columns:\n",
    "    if col in meta_list:\n",
    "        continue\n",
    "    if (dff[col].isna().sum()/len(dff))>0.02:\n",
    "        \n",
    "        if 'longterm' not in col:\n",
    "            print(col, dff[col].isna().sum())\n",
    "            drop_list.append(col)\n",
    "        \n",
    "print(f'remove covariates with more than 2% data unavailable')\n",
    "dff = dff.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e758d666-b097-4b49-a495-7cbfb18b2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff.drop(columns=['geometry'])\n",
    "dff.to_csv(f'{folder}/data/001_covar_overlayed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b059522-c9e0-408c-95fa-a81d30b1e2d7",
   "metadata": {},
   "source": [
    "### Assign spatial blocking ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e54765e2-2126-4c0a-ae5a-17fb97d7286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "# create a tiling system first\n",
    "from eumap.parallel import TilingProcessing\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# raster_layer_fn = f'http://192.168.1.30:8333/ai4sh-landmasked/ndvi/ndvi_glad.landast.ard2.seasconv.m.yearly_p75_30m_s_20220101_20221231_eu_epsg.3035_v20231127.tif'\n",
    "# ds = rasterio.open(raster_layer_fn)\n",
    "# tiles_size = ds.transform[0] * 1000 # 30m -> 30km\n",
    "\n",
    "# tiling_system = TilingProcessing.generate_tiles(tiles_size, extent=ds.bounds, crs=ds.crs, raster_layer_fn=raster_layer_fn)\n",
    "# tiling_system = tiling_system.to_crs(\"EPSG:4326\")\n",
    "# tiling_system = tiling_system[['tile_id','geometry']]\n",
    "# tiling_system.to_file('/mnt/inca/soc_eu_model/data/000_tile_eu4326.gpkg',  driver=\"GPKG\")\n",
    "# # tiling_system[tiling_system['raster_mode_count'] > 0].to_file('/mnt/inca/soc_eu_model/data/000_tile_eu4326.gpkg.gpkg',  driver=\"GPKG\")\n",
    "\n",
    "df = pd.read_csv(f'{folder}/data/test_covar_overlayed.csv',low_memory=False)\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "from shapely.geometry import Point\n",
    "tiles = gpd.read_file(f'{folder}/data/000_tile_eu4326.gpkg')\n",
    "\n",
    "gdf.crs = tiles.crs\n",
    "joined_gdf = gpd.sjoin(gdf, tiles, how=\"left\", op='within')\n",
    "joined_gdf = joined_gdf.drop(columns=['index_right','geometry'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2c1d59b-6ae8-4149-afbb-558db51b7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.to_csv(f'{folder}/data/test_covar_overlayed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d500847-b809-46d9-be79-2507c68b405b",
   "metadata": {},
   "source": [
    "### epsg 4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3182b-ea01-4a72-b342-130b84a9404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # epsg 4326 overlay\n",
    "# co4326 = covar.loc[covar['epsg']=='4326']\n",
    "# path_ori = [i.split('/')[-1][0:-4] for i in co4326['path']]\n",
    "\n",
    "# for year in np.arange(2000,2024,1):\n",
    "#     pathl = []\n",
    "#     namel = []\n",
    "#     for index,row in co4326.iterrows():\n",
    "#         paths, names = generate_overlay_path(row, year)\n",
    "#         pathl.extend(paths)\n",
    "#         namel.extend(names)\n",
    "#         # path4326.extend(generate_overlay_path(row,year))\n",
    "        \n",
    "# #     for i in pathl:\n",
    "# #         check_path(i)\n",
    "        \n",
    "#     path_stem = [i.split('/')[-1][0:-4] for i in pathl]\n",
    "#     namel = [i.split('/')[-1][0:-4] for i in namel]\n",
    "#     name_mapping = dict(zip(path_stem,namel))\n",
    "    \n",
    "#     df_overlay = df4326.loc[df4326['time']==year]\n",
    "#     if len(df_overlay)==0:\n",
    "#         print(f'no data for year {year}')\n",
    "#         continue\n",
    "        \n",
    "#     ttprint(f'start overlaying for year {str(int(year))}, size: {len(df_overlay)}')\n",
    "#     pathl = [Path(ii) for ii in pathl]\n",
    "#     dfo = SpaceOverlay(df_overlay, fn_layers=pathl, max_workers=90, verbose=False)\n",
    "#     temp = dfo.run()\n",
    "    \n",
    "#     temp = temp.rename(columns=name_mapping)\n",
    "#     temp=temp.drop(columns=['overlay_id'])\n",
    "#     # temp = pd.read_csv(f'/mnt/inca/soc_eu_model/overlay_intermediate/dft_{str(int(tt))}_4326.csv',index=False)\n",
    "#     temp.to_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}_4326.csv',index=False)\n",
    "#     ttprint(f'finish overlaying for year {str(int(year))}')\n",
    "\n",
    "\n",
    "# # read in 4326 datasets\n",
    "# tl = []\n",
    "\n",
    "# for year in np.arange(2000,2023,1):\n",
    "#     temp = pd.read_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}_4326.csv',low_memory=False)\n",
    "#     temp = temp.rename(columns=name_mapping)\n",
    "#     print(f'{year}, {len(temp.columns)}, {len(temp)}')\n",
    "#     tl.append(temp)\n",
    "\n",
    "# df4326 = pd.concat(tl)\n",
    "# print(f'whole 4326, {len(df4326.columns)}, {len(df4326)}')\n",
    "\n",
    "\n",
    "# # merge to merge\n",
    "# cols_list = df3035.columns.values.tolist()\n",
    "# meta_list = ['lat', 'lon', 'oc', 'ph_h2o', 'ph_cacl2', 'bulk_density', 'clay','silt', 'sand', 'caco3','N',\n",
    "#              'K', 'P', 'CEC', 'EC', 'nuts0', 'time','hzn_top','hzn_btm','ref','sample_id']\n",
    "# dff = pd.merge(df3035,df4326,on = meta_list, how='inner')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
