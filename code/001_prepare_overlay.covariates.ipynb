{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034dea9b-c505-4ea5-8347-a5875d2b21b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opengeohub/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.3-CAPI-1.17.3) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocess as mp\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from eumap.misc import find_files, nan_percentile, GoogleSheet, ttprint\n",
    "from eumap.raster import read_rasters, save_rasters\n",
    "from eumap.mapper import SpaceOverlay\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from minio import Minio\n",
    "import rasterio\n",
    "import pyproj\n",
    "from shapely.geometry import Point\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "# os.environ['PROJ_LIB'] = '/opt/conda/share/proj'\n",
    "folder = '/mnt/inca/soc_eu_model'\n",
    "# /home/opengeohub/.local/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56ee2b-daef-45d1-9310-f67a7c41dc95",
   "metadata": {},
   "source": [
    "### check if what is need to be overlayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbf9596-28b0-423a-b078-142146f8f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "from geopandas import gpd\n",
    "\n",
    "# def get_data_to_be_overlayed(whole=False):\n",
    "#     df4326 = gpd.read_file(f'{folder}/data/soil_overlay.4326.gpkg')\n",
    "#     df3035 = gpd.read_file(f'{folder}/data/soil_overlay.3035.gpkg')\n",
    "    \n",
    "#     keys = ['sample_id', 'lat', 'lon', 'time', 'hzn_top', 'hzn_btm', 'ref']\n",
    "#     if whole:\n",
    "#         return df4326,df3035\n",
    "#     else:\n",
    "#         new4326 = gpd.read_file(f'{folder_path}/data/soil_overlay.4326_v2.gpkg')\n",
    "#         merge4326 = pd.merge(df4326, new4326, on=keys, how='outer', indicator=True)\n",
    "#         different_4326 = merge4326[merge4326['_merge'] != 'both']\n",
    "        \n",
    "#         new3035 = gpd.read_file(f'{folder_path}/data/soil_overlay.3035_v2.gpkg')\n",
    "#         merge3035 = pd.merge(df3035, new3035, on=keys, how='outer', indicator=True)\n",
    "#         different_3035 = merge3035[merge3035['_merge'] != 'both']\n",
    "#         return different_4326,different_3035\n",
    "\n",
    "# df4326, df3035 = get_data_to_be_overlayed(whole=True)     \n",
    "\n",
    "# # create gpkg\n",
    "\n",
    "df = pd.read_csv(f'{folder}/data/000_soil.full_qa.controlled.csv', low_memory=False)\n",
    "geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "\n",
    "df4326 = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "df3035 = df4326.to_crs(\"EPSG:3035\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1960bf-048d-456c-94b2-dacc0cc2a9cc",
   "metadata": {},
   "source": [
    "### generate overlay links\n",
    "- read in the files specified by Google Sheet\n",
    "- convert the files into readable linkes for overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4113b0ed-bb82-4c0d-a6e8-0f5ccb9e949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/inca/soc_eu_model/SOC-EU/code/src/eumap/eumap/misc.py:363: ResourceWarning: unclosed <ssl.SSLSocket fd=61, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.17.0.5', 52296), raddr=('142.250.179.170', 443)>\n",
      "  self._read_gsheet()\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# read in potential usable overlay files\n",
    "key_file = '/mnt/inca/soc_eu_model/gaia-319808-913d36b5fca4.json'\n",
    "url = 'https://docs.google.com/spreadsheets/d/1eIoPAvWM5jrhLrr25jwguAIR0YxOh3f5-CdXwpcOIz8/edit#gid=0'\n",
    "\n",
    "gsheet = GoogleSheet(key_file, url)\n",
    "covar = gsheet.covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df5670e-bdfc-4e4b-a08a-b409349d4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate file paths by year, and check if the urls are valid\n",
    "def generate_overlay_path(row,year):\n",
    "    # determine if static variable\n",
    "    if row['temporal resolution'] == 'static':\n",
    "        return [row['path']],[row['path']]\n",
    "    \n",
    "    # determine if the year is ahead of the availibility of the variable\n",
    "    if year>int(row['end year']):\n",
    "        year = int(row['end year'])\n",
    "    \n",
    "    # determine if it's an annual variable or (bi)monthly variable\n",
    "    if '{start_m}' not in row['path']:\n",
    "        output_paths = [row['path'].replace('{year}',f'{int(year)}')]\n",
    "    else:\n",
    "        output_paths = []\n",
    "        start_list = row['start_m'].split(', ')\n",
    "        end_list = row['end_m'].split(', ')\n",
    "        output_paths = [row['path'].replace('{year}',f'{int(year)}').replace('{start_m}',start_list[i]).replace('{end_m}',end_list[i]) for i in range(len(end_list))]\n",
    "    \n",
    "    if '{perc}' in row['path']:\n",
    "        perc_list = row['perc'].split(',')\n",
    "        output_paths = [p.replace('{perc}', perc) for p in output_paths for perc in perc_list]\n",
    "        \n",
    "    if (row['leap year'] == '1') & (year%4==0):\n",
    "        output_paths = [p.replace('0228', '0229') if '0228' in p else p for p in output_paths]\n",
    "    \n",
    "    return output_paths, [i.replace(str(int(year)),'{year}') for i in output_paths]\n",
    "    \n",
    "def check_path(url):\n",
    "    try:\n",
    "        response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "        # Check if the status code is not 200 (OK). You might want to specifically check for 404 or other error codes.\n",
    "        if response.status_code == 404:\n",
    "            print(f\"{url} returned HTTP 404 Not Found\")\n",
    "            return url\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"{url} returned HTTP {response.status_code}\")\n",
    "            return url\n",
    "        return None  # URL is fine (HTTP 200), or you might want to handle redirections (HTTP 3xx) separately if needed.\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {str(e)}\")\n",
    "        return url\n",
    "    \n",
    "# # check function validity\n",
    "# # generate paths\n",
    "# paths = []\n",
    "# for index,row in covar.iterrows():\n",
    "#     paths.extend(generate_overlay_path(row,2000))\n",
    "    \n",
    "pathl = []\n",
    "namel = []\n",
    "year = 2022\n",
    "for index,row in covar.iterrows():\n",
    "    paths, names = generate_overlay_path(row, year)\n",
    "    pathl.extend(paths)\n",
    "    namel.extend(names)\n",
    "    \n",
    "for i in pathl:\n",
    "    check_path(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de10f7-b69b-4e7e-b644-bfe3e70c52fd",
   "metadata": {},
   "source": [
    "### overlay year by year\n",
    "- divide soil data by year\n",
    "- overlay the soil data in each year with corresponding covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2ca97e-5ab6-4de1-aaff-893de6117eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:07:22] start overlaying for year 2000, size: 26304\n",
      "[19:11:29] finish overlaying for year 2000\n",
      "[19:11:29] start overlaying for year 2001, size: 12777\n",
      "[19:16:17] finish overlaying for year 2001\n",
      "[19:16:17] start overlaying for year 2002, size: 11718\n",
      "[19:21:15] finish overlaying for year 2002\n",
      "[19:21:15] start overlaying for year 2003, size: 13497\n",
      "[19:28:05] finish overlaying for year 2003\n",
      "[19:28:05] start overlaying for year 2004, size: 7649\n",
      "[19:32:59] finish overlaying for year 2004\n",
      "[19:32:59] start overlaying for year 2005, size: 12987\n",
      "[19:38:16] finish overlaying for year 2005\n",
      "[19:38:16] start overlaying for year 2006, size: 13075\n",
      "[19:42:40] finish overlaying for year 2006\n",
      "[19:42:40] start overlaying for year 2007, size: 18561\n",
      "[19:55:47] finish overlaying for year 2007\n",
      "[19:55:47] start overlaying for year 2008, size: 11983\n",
      "[20:30:34] finish overlaying for year 2008\n",
      "[20:30:34] start overlaying for year 2009, size: 25942\n",
      "[21:26:58] finish overlaying for year 2009\n",
      "[21:26:58] start overlaying for year 2010, size: 12330\n",
      "[21:33:12] finish overlaying for year 2010\n",
      "[21:33:12] start overlaying for year 2011, size: 15374\n",
      "[21:40:25] finish overlaying for year 2011\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# epsg 3035 overlay\n",
    "co3035 = covar.loc[covar['epsg']=='3035']\n",
    "\n",
    "for year in np.arange(2000,2012,1):\n",
    "    pathl = []\n",
    "    namel = []\n",
    "    for index,row in co3035.iterrows():\n",
    "        paths, names = generate_overlay_path(row, year)\n",
    "        pathl.extend(paths)\n",
    "        namel.extend(names)\n",
    "        # path3035.extend(generate_overlay_path(row,year))\n",
    "    \n",
    "    path_stem = [i.split('/')[-1][0:-4] for i in pathl]\n",
    "    namel = [i.split('/')[-1][0:-4] for i in namel]\n",
    "    name_mapping = dict(zip(path_stem,namel))\n",
    "    \n",
    "    df_overlay = df3035.loc[df3035['time']==year]\n",
    "    if len(df_overlay)==0:\n",
    "        print(f'no data for year {year}')\n",
    "        continue\n",
    "        \n",
    "    ttprint(f'start overlaying for year {str(int(year))}, size: {len(df_overlay)}')\n",
    "    pathl = [Path(ii) for ii in pathl]\n",
    "    dfo = SpaceOverlay(df_overlay, fn_layers=pathl, max_workers=90, verbose=False)\n",
    "    temp = dfo.run()\n",
    "    \n",
    "    temp = temp.rename(columns=name_mapping)\n",
    "    temp=temp.drop(columns=['overlay_id'])\n",
    "    # temp = pd.read_csv(f'/mnt/inca/soc_eu_model/overlay_intermediate/dft_{str(int(tt))}_3035.csv',index=False)\n",
    "    temp.to_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}_3035.csv',index=False)\n",
    "    ttprint(f'finish overlaying for year {str(int(year))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10fc8dd-119a-4452-aa18-b95dce61c3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:57:55] start overlaying for year 2000, size: 26304\n",
      "[17:58:24] finish overlaying for year 2000\n",
      "[17:58:24] start overlaying for year 2001, size: 12777\n",
      "[17:58:45] finish overlaying for year 2001\n",
      "[17:58:45] start overlaying for year 2002, size: 11718\n",
      "[17:59:06] finish overlaying for year 2002\n",
      "[17:59:06] start overlaying for year 2003, size: 13497\n",
      "[17:59:28] finish overlaying for year 2003\n",
      "[17:59:28] start overlaying for year 2004, size: 7649\n",
      "[17:59:46] finish overlaying for year 2004\n",
      "[17:59:46] start overlaying for year 2005, size: 12987\n",
      "[18:00:05] finish overlaying for year 2005\n",
      "[18:00:05] start overlaying for year 2006, size: 13075\n",
      "[18:00:32] finish overlaying for year 2006\n",
      "[18:00:32] start overlaying for year 2007, size: 18561\n",
      "[18:01:04] finish overlaying for year 2007\n",
      "[18:01:04] start overlaying for year 2008, size: 11983\n",
      "[18:01:44] finish overlaying for year 2008\n",
      "[18:01:44] start overlaying for year 2009, size: 25942\n",
      "[18:02:22] finish overlaying for year 2009\n",
      "[18:02:22] start overlaying for year 2010, size: 12330\n",
      "[18:03:41] finish overlaying for year 2010\n",
      "[18:03:41] start overlaying for year 2011, size: 15374\n",
      "[18:04:22] finish overlaying for year 2011\n",
      "[18:04:22] start overlaying for year 2012, size: 26062\n",
      "[18:04:57] finish overlaying for year 2012\n",
      "[18:04:57] start overlaying for year 2013, size: 21428\n",
      "[18:05:24] finish overlaying for year 2013\n",
      "[18:05:24] start overlaying for year 2014, size: 16922\n",
      "[18:05:53] finish overlaying for year 2014\n",
      "[18:05:53] start overlaying for year 2015, size: 33628\n",
      "[18:06:48] finish overlaying for year 2015\n",
      "[18:06:48] start overlaying for year 2016, size: 16177\n",
      "[18:07:19] finish overlaying for year 2016\n",
      "[18:07:19] start overlaying for year 2017, size: 15843\n",
      "[18:07:46] finish overlaying for year 2017\n",
      "[18:07:46] start overlaying for year 2018, size: 42918\n",
      "[18:08:33] finish overlaying for year 2018\n",
      "[18:08:33] start overlaying for year 2019, size: 15088\n",
      "[18:08:55] finish overlaying for year 2019\n",
      "[18:08:55] start overlaying for year 2020, size: 10200\n",
      "[18:09:16] finish overlaying for year 2020\n",
      "[18:09:16] start overlaying for year 2021, size: 10886\n",
      "[18:09:37] finish overlaying for year 2021\n",
      "[18:09:37] start overlaying for year 2022, size: 3294\n",
      "[18:09:48] finish overlaying for year 2022\n",
      "no data for year 2023\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# epsg 4326 overlay\n",
    "co4326 = covar.loc[covar['epsg']=='4326']\n",
    "path_ori = [i.split('/')[-1][0:-4] for i in co4326['path']]\n",
    "\n",
    "for year in np.arange(2000,2024,1):\n",
    "    pathl = []\n",
    "    namel = []\n",
    "    for index,row in co4326.iterrows():\n",
    "        paths, names = generate_overlay_path(row, year)\n",
    "        pathl.extend(paths)\n",
    "        namel.extend(names)\n",
    "        # path4326.extend(generate_overlay_path(row,year))\n",
    "        \n",
    "#     for i in pathl:\n",
    "#         check_path(i)\n",
    "        \n",
    "    path_stem = [i.split('/')[-1][0:-4] for i in pathl]\n",
    "    namel = [i.split('/')[-1][0:-4] for i in namel]\n",
    "    name_mapping = dict(zip(path_stem,namel))\n",
    "    \n",
    "    df_overlay = df4326.loc[df4326['time']==year]\n",
    "    if len(df_overlay)==0:\n",
    "        print(f'no data for year {year}')\n",
    "        continue\n",
    "        \n",
    "    ttprint(f'start overlaying for year {str(int(year))}, size: {len(df_overlay)}')\n",
    "    pathl = [Path(ii) for ii in pathl]\n",
    "    dfo = SpaceOverlay(df_overlay, fn_layers=pathl, max_workers=90, verbose=False)\n",
    "    temp = dfo.run()\n",
    "    \n",
    "    temp = temp.rename(columns=name_mapping)\n",
    "    temp=temp.drop(columns=['overlay_id'])\n",
    "    # temp = pd.read_csv(f'/mnt/inca/soc_eu_model/overlay_intermediate/dft_{str(int(tt))}_4326.csv',index=False)\n",
    "    temp.to_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}_4326.csv',index=False)\n",
    "    ttprint(f'finish overlaying for year {str(int(year))}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdaee12-dad4-4f62-a12c-3e4f1981495b",
   "metadata": {},
   "source": [
    "### assemble the overlayed annual datasets\n",
    "- read in the overlayed soil data (with covariates) for each year\n",
    "- combine them into a whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82ae897-faee-42eb-8d23-b34806425c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000, 264, 26304\n",
      "2001, 264, 12777\n",
      "2002, 264, 11718\n",
      "2003, 264, 13497\n",
      "2004, 264, 7649\n",
      "2005, 264, 12987\n",
      "2006, 264, 13075\n",
      "2007, 264, 18561\n",
      "2008, 264, 11983\n",
      "2009, 264, 25942\n",
      "2010, 264, 12330\n",
      "2011, 264, 15374\n",
      "2012, 264, 26062\n",
      "2013, 264, 21428\n",
      "2014, 264, 16922\n",
      "2015, 264, 33628\n",
      "2016, 264, 16177\n",
      "2017, 264, 15843\n",
      "2018, 264, 42918\n",
      "2019, 264, 15088\n",
      "2020, 264, 10200\n",
      "2021, 264, 10886\n",
      "2022, 264, 3294\n",
      "whole 3035, 264, 394643\n",
      "2000, 197, 26304\n",
      "2001, 197, 12777\n",
      "2002, 197, 11718\n",
      "2003, 197, 13497\n",
      "2004, 197, 7649\n",
      "2005, 197, 12987\n",
      "2006, 197, 13075\n",
      "2007, 197, 18561\n",
      "2008, 197, 11983\n",
      "2009, 197, 25942\n",
      "2010, 197, 12330\n",
      "2011, 197, 15374\n",
      "2012, 197, 26062\n",
      "2013, 197, 21428\n",
      "2014, 197, 16922\n",
      "2015, 197, 33628\n",
      "2016, 197, 16177\n",
      "2017, 197, 15843\n",
      "2018, 197, 42918\n",
      "2019, 197, 15088\n",
      "2020, 197, 10200\n",
      "2021, 197, 10886\n",
      "2022, 197, 3294\n",
      "whole 4326, 197, 394643\n"
     ]
    }
   ],
   "source": [
    "## read in 3035 datasets\n",
    "tl = []\n",
    "for year in np.arange(2000,2023,1):\n",
    "    temp = pd.read_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}_3035.csv',low_memory=False)\n",
    "    print(f'{year}, {len(temp.columns)}, {len(temp)}')\n",
    "    tl.append(temp)\n",
    "\n",
    "\n",
    "df3035 = pd.concat(tl)\n",
    "print(f'whole 3035, {len(df3035.columns)}, {len(df3035)}')\n",
    "\n",
    "# read in 4326 datasets\n",
    "tl = []\n",
    "name_mapping = {'wv_mcd19a2v061.seasconv_m_1km_s_{year}0201_{year}0229_go_epsg.4326_v20230619':'wv_mcd19a2v061.seasconv_m_1km_s_{year}0201_{year}0228_go_epsg.4326_v20230619',\n",
    "                'wv_mcd19a2v061.seasconv_sd_1km_s_{year}0201_{year}0229_go_epsg.4326_v20230619':'wv_mcd19a2v061.seasconv_sd_1km_s_{year}0201_{year}0228_go_epsg.4326_v20230619'}\n",
    "\n",
    "for year in np.arange(2000,2023,1):\n",
    "    temp = pd.read_csv(f'{folder}/overlay_intermediate/dft_{str(int(year))}_4326.csv',low_memory=False)\n",
    "    temp = temp.rename(columns=name_mapping)\n",
    "    print(f'{year}, {len(temp.columns)}, {len(temp)}')\n",
    "    tl.append(temp)\n",
    "\n",
    "df4326 = pd.concat(tl)\n",
    "print(f'whole 4326, {len(df4326.columns)}, {len(df4326)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a6014d-428a-49b2-93a4-5cbe99eefa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge\n",
    "cols3035 = df3035.columns\n",
    "cols4326 = df4326.columns\n",
    "shared_columns = cols3035.intersection(cols4326)\n",
    "shared_columns\n",
    "\n",
    "# # merge to merge\n",
    "# cols_list = df3035.columns.values.tolist()\n",
    "# meta_list = ['lat', 'lon', 'oc', 'ph_h2o', 'ph_cacl2', 'bulk_density', 'clay','silt', 'sand', 'caco3','N',\n",
    "#              'K', 'P', 'CEC', 'EC', 'nuts0', 'time','hzn_top','hzn_btm','ref','sample_id']\n",
    "# dff = pd.merge(df3035,df4326,on = meta_list, how='inner')\n",
    "\n",
    "# concat to merge\n",
    "cols_list = df3035.columns.values.tolist()\n",
    "concat_list = [i for i in cols_list if i not in shared_columns]\n",
    "df3035_2 = df3035[concat_list]\n",
    "\n",
    "dff = pd.concat([df4326,df3035_2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb140667-256c-4a4b-95bc-0c72733c32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lcv_wilderness_li2022.human.footprint_p_1km_s0..0cm_{year}_v16022022 394643\n",
      "bioclim.var_chelsa.gddlgd0_m_1km_s_19810101_20101231_eu_epsg.3035_v20230822 342902\n",
      "bioclim.var_chelsa.gdgfgd0_m_1km_s_19810101_20101231_eu_epsg.3035_v20230822 342902\n",
      "bioclim.var_chelsa.gddlgd5_m_1km_s_19810101_20101231_eu_epsg.3035_v20230822 36851\n",
      "bioclim.var_chelsa.gdgfgd5_m_1km_s_19810101_20101231_eu_epsg.3035_v20230822 36851\n",
      "bioclim.var_chelsa.lgd_m_1km_s_19810101_20101231_eu_epsg.3035_v20230822 258826\n",
      "bioclim.var_chelsa.swe_m_1km_s_19810101_20101231_eu_epsg.3035_v20230822 343069\n",
      "remove covariates with more than 2% data unavailable\n"
     ]
    }
   ],
   "source": [
    "### check covariates availability\n",
    "drop_list = []\n",
    "for col in dff.columns:\n",
    "    if col in shared_columns:\n",
    "        continue\n",
    "    if (dff[col].isna().sum()/len(dff))>0.02:\n",
    "        print(col, dff[col].isna().sum())\n",
    "        drop_list.append(col)\n",
    "        \n",
    "print(f'remove covariates with more than 2% data unavailable')\n",
    "dff = dff.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e758d666-b097-4b49-a495-7cbfb18b2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.to_csv(f'{folder}/data/test_covar_overlayed.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b059522-c9e0-408c-95fa-a81d30b1e2d7",
   "metadata": {},
   "source": [
    "### Assign spatial blocking ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54765e2-2126-4c0a-ae5a-17fb97d7286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tiling system first\n",
    "from eumap.parallel import TilingProcessing\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "from shapely.geometry import box\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# raster_layer_fn = f'http://192.168.1.30:8333/ai4sh-landmasked/ndvi/ndvi_glad.landast.ard2.seasconv.m.yearly_p75_30m_s_20220101_20221231_eu_epsg.3035_v20231127.tif'\n",
    "# ds = rasterio.open(raster_layer_fn)\n",
    "# tiles_size = ds.transform[0] * 1000 # 30m -> 30km\n",
    "\n",
    "# tiling_system = TilingProcessing.generate_tiles(tiles_size, extent=ds.bounds, crs=ds.crs, raster_layer_fn=raster_layer_fn)\n",
    "# tiling_system = tiling_system.to_crs(\"EPSG:4326\")\n",
    "# tiling_system = tiling_system[['tile_id','geometry']]\n",
    "# tiling_system.to_file('/mnt/inca/soc_eu_model/data/000_tile_eu4326.gpkg',  driver=\"GPKG\")\n",
    "# # tiling_system[tiling_system['raster_mode_count'] > 0].to_file('/mnt/inca/soc_eu_model/data/000_tile_eu4326.gpkg.gpkg',  driver=\"GPKG\")\n",
    "\n",
    "df = pd.read_csv(f'{folder}/data/test_covar_overlayed.csv',low_memory=False)\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "\n",
    "from shapely.geometry import Point\n",
    "tiles = gpd.read_file(f'{folder}/data/000_tile_eu4326.gpkg')\n",
    "\n",
    "gdf.crs = tiles.crs\n",
    "joined_gdf = gpd.sjoin(gdf, tiles, how=\"left\", op='within')\n",
    "joined_gdf = joined_gdf.drop(columns=['index_right','geometry'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2c1d59b-6ae8-4149-afbb-558db51b7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.to_csv('/mnt/inca/soc_eu_model/data/test_covar_overlayed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3182b-ea01-4a72-b342-130b84a9404f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
