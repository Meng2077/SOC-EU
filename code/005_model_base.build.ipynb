{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9305d568-6e58-4aea-9467-5eb3341f7d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/opengeohub/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.3-CAPI-1.17.3) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GroupKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_521603/1132020213.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mspatial_cv_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tile_id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGroupKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'GroupKFold' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from eumap.misc import find_files, nan_percentile, GoogleSheet, ttprint\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tool_kit import calc_ccc, accuracy_plot\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # model parameters\n",
    "# import json\n",
    "# with open('/mnt/inca/soc_eu_model/data/006_params_annual.json', 'r') as file:\n",
    "#     params = json.load(file)\n",
    "\n",
    "folder = '/mnt/inca/soc_eu_model'\n",
    "df = pd.read_csv(f'{folder}/data/005.0_train.pnts_soc.csv',low_memory=False)\n",
    "\n",
    "# covariates\n",
    "with open(f'{folder}/SOC-EU/features/002_selected.covar_rank.freq.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "covs = [line.strip() for line in lines]\n",
    "\n",
    "# target\n",
    "train = df.loc[df['oc'].notna()]\n",
    "train = train.loc[train['oc']>5]\n",
    "train = train.loc[train['ref']!='nl.bis'] # show weird patterns\n",
    "train.loc[:,'oc_log1p'] = np.log1p(train['oc'])\n",
    "tgt = 'oc_log1p'\n",
    "\n",
    "spatial_cv_column = 'tile_id'\n",
    "cv = GroupKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe135b57-217a-4e97-9c03-14455c42e384",
   "metadata": {},
   "source": [
    "### Conduct hyperparameter tuning for different base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a35eda-9324-41bf-89a5-879e0ce6f64a",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb95e2d-9c21-4c55-ac64-df25d5b96070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GroupKFold\n",
    "from joblib\n",
    "\n",
    "# https://zillow.github.io/quantile-forest/user_guide/fit_predict.html#random-forest-predictions\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 500, 800, 1000],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'max_features': [0.3, 0.5, 0.7, 'log2', 'sqrt'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "tune_rf = HalvingGridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90, \n",
    "    cv=cv,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ttprint(f'start parameter fine tuning for rf, training size: {len(train)}')\n",
    "tune_rf.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_rf.best_params_)\n",
    "joblib.dump(tune_rf.best_params_, f'{folder}/SOC-EU/model/001_best.params_rf.joblib')\n",
    "joblib.dump(tune_rf.best_estimator_, f'{folder}/SOC-EU/model/002_model_rf.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285870b8-2634-47d7-bbc5-839221ee8dca",
   "metadata": {},
   "source": [
    "#### lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4c611-7eeb-49c5-a2ee-ef1c7858c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/index.html\n",
    "import lightgbm as lgb\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV, GroupKFold\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "# [LightGBM] [Fatal] Do not support special JSON characters in feature name.\n",
    "def clean_feature_names(df, covs):\n",
    "    df.columns = df.columns.str.replace(r'[^\\w]', '_', regex=True)\n",
    "    clean_covs = [re.sub(r'[^\\w]', '_', i) for i in covs]\n",
    "    return df, clean_covs\n",
    "\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'dart'], # traditional Gradient Boosting Decision Tree VS. dropouts meet Multiple Additive Regression Trees (prevent overfitting)\n",
    "    'num_leaves': [31, 50, 80], # number of leaved in trees\n",
    "    'max_depth': [-1, 10, 20], # depth of a tree\n",
    "    'learning_rate': [0.01, 0.1], # shrinkage or step size, this parameter controls the impact of each tree on the final outcome\n",
    "    'n_estimators': [100, 500, 800], # number of boosting rounds\n",
    "    'subsample': [0.6, 0.8, 1.0], # fraction of samples to be used for each tree\n",
    "    'min_child_samples': [10,20,30], # minimum number of data points needed in a leaf\n",
    "    'verbose': [-1]\n",
    "}\n",
    "\n",
    "# HalvingGridSearchCV for tuning\n",
    "tune_lgbm = HalvingGridSearchCV(\n",
    "    estimator=lgb.LGBMRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=cv,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Clean feature names\n",
    "train_clean, covs_clean = clean_feature_names(train, covs)\n",
    "\n",
    "# Parameter tuning\n",
    "ttprint(f'start parameter fine tuning for lightGBM, training size: {len(train_clean)}')\n",
    "tune_lgbm.fit(train_clean[covs_clean], train_clean[tgt], groups=train[spatial_cv_column])\n",
    "ttprint(\"Finish fine tuning\\nBest parameters found: \", tune_lgbm.best_params_)\n",
    "joblib.dump(tune_lgbm.best_params_, f'{folder}/SOC-EU/model/003_best.params_lgbm.joblib')\n",
    "joblib.dump(tune_lgbm.best_estimator_, f'{folder}/SOC-EU/model/004_model_lgbm.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0ce5f-c2a9-413f-b563-bba05291b9e9",
   "metadata": {},
   "source": [
    "#### ANN with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94f0255-ca5f-4b0a-8a24-c7a6d5895be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Convert DataFrame to PyTorch tensors\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(train[covs])\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "y_scaled = target_scaler.fit_transform(train[[tgt]]).reshape(-1)\n",
    "\n",
    "X = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y = torch.tensor(y_scaled, dtype=torch.float32).reshape(-1, 1)\n",
    "groups = train[spatial_cv_column].values\n",
    "\n",
    "# Define PyTorch model class\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, units, layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        layers_list = [nn.Linear(X.shape[1], units), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        for _ in range(1, layers):\n",
    "            layers_list += [nn.Linear(units, units), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        layers_list.append(nn.Linear(units, 1))\n",
    "        self.net = nn.Sequential(*layers_list)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Skorch wrapper\n",
    "def skorch_model(units=64, layers=1, dropout_rate=0.2, learning_rate=0.001):\n",
    "    return NeuralNetRegressor(\n",
    "        RegressionModel,\n",
    "        module__units=units,\n",
    "        module__layers=layers,\n",
    "        module__dropout_rate=dropout_rate,\n",
    "        max_epochs=10,  # This value will be overwritten by GridSearchCV\n",
    "        lr=learning_rate,\n",
    "        optimizer=optim.Adam,\n",
    "        criterion=nn.MSELoss,\n",
    "        batch_size=64,  # This value will be overwritten by GridSearchCV\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "param_grid = {\n",
    "    'module__units': [64, 128, 256],\n",
    "    'module__layers': [2,4,6],\n",
    "    'module__dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'lr': [0.0005, 0.001, 0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "    'batch_size': [64, 128]\n",
    "}\n",
    "\n",
    "ttprint('start grid search')\n",
    "cv = GroupKFold(n_splits=3)\n",
    "grid = GridSearchCV(estimator=skorch_model(), param_grid=param_grid, n_jobs=-1, cv=cv, scoring='neg_mean_squared_error')\n",
    "grid_result = grid.fit(X, y, groups=groups)\n",
    "ttprint('finish tuning')\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "joblib.dump(grid_result.best_params_, f'{folder}/SOC-EU/model/005_best.params_ann.joblib')\n",
    "joblib.dump(grid_result.best_estimator_,  f'{folder}/SOC-EU/model/006_model_ann.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f17f8-cba1-44a9-ac1f-3514db9c8060",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8d6dd3-ee4f-477f-8cd9-dc522edf2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "\n",
    "# find the best boundary (hyperplane) that separates data points of different classes in the feature space\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],  # regularization parameter, lower values of C lead to a smaller margin in the separating hyperplane\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # type of hyperplane used to separate the data\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10],  # the influence of a single training example\n",
    "    'degree': [2, 3, 4]  # degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "}\n",
    "\n",
    "# Set up the HalvingGridSearchCV for SVM\n",
    "tune_svm = HalvingGridSearchCV(\n",
    "    estimator=SVR(),  \n",
    "    param_grid=param_grid_svm,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90,  \n",
    "    cv=cv, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Assuming you have defined train, covs, tgt, and spatial_cv_column appropriately\n",
    "print(f'start parameter fine tuning for SVM, training size: {len(train)}')\n",
    "tune_svm.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "print(\"Finish fine tuning\\nBest parameters found: \", tune_svm.best_params_)\n",
    "\n",
    "# Save the best parameters and the best estimator\n",
    "joblib.dump(tune_svm.best_params_, f'{folder}/SOC-EU/model/009_best.params_svm.joblib')\n",
    "joblib.dump(tune_svm.best_estimator_, f'{folder}/SOC-EU/model/010_model_svm.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b365c5-dc81-4fd1-a13c-e07c6b3ba514",
   "metadata": {},
   "source": [
    "### lrboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da035a-5f5c-4dd1-807a-b798b36aa23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/lrboost/\n",
    "\n",
    "from lrboost import LRBoostRegressor\n",
    "\n",
    "ttprint(f'fit linear regression Boost regressor')\n",
    "lrb = LRBoostRegressor(primary_model=RidgeCV(), secondary_model=RandomForestRegressor())\n",
    "lrb = LRBoostRegressor.fit(train[covs], train[tgt])\n",
    "ttprint(f'finish fitting linear regression Boost regressor')\n",
    "\n",
    "joblib.dump(lrb, f'{folder}/SOC-EU/model/011_model_lrb.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2c86f-3d13-456c-9452-a9438c9b1544",
   "metadata": {},
   "source": [
    "#### Cubist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a8008-10f8-47e5-9607-23fc2120375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cubist import Cubist\n",
    "# https://pypi.org/project/cubist/\n",
    "# rule-based predictive model\n",
    "param_cubist = {\n",
    "    'n_rules': [100, 300, 500], # number of rules to be generated\n",
    "    'n_committees': [1, 5, 10], # committee: ensembles of models\n",
    "    'neighbors': [None, 3, 6, 9], # number of nearest neighbors to use when making a prediction\n",
    "    'unbiased': [False, True], # whether or not to use an unbiased method of rule generation\n",
    "    'extrapolation': [0.02, 0.05], # limits the extent to which predictions can extrapolate beyond the range of the training data, a fraction of the total range of the target variable\n",
    "    'sample': [None, 0.1, 0.5], # fraction of the training data used in building each model\n",
    "    'cv': [10]\n",
    "}\n",
    "\n",
    "tune_cubist = HalvingGridSearchCV(\n",
    "    estimator=Cubist(),\n",
    "    param_grid=param_cubist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=90,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "# Start fine-tuning process\n",
    "ttprint('start fine tuning cubist')\n",
    "tune_cubist.fit(train[covs], train[tgt], groups=train[spatial_cv_column])\n",
    "ttprint('finish fitting')\n",
    "\n",
    "print(\"Best parameters:\", tune_cubist.best_params_)\n",
    "joblib.dump(tune_cubist.best_params_, f'{folder}/SOC-EU/model/007_best.params_ann.joblib')\n",
    "joblib.dump(tune_cubist.best_estimator_,  f'{folder}/SOC-EU/model/008_model_cubist.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
